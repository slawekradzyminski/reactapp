{
  "id": "2016-09-09-testops-2-testing-in-production",
  "title": "TestOps - Testing in Production",
  "date": "2016-09-09",
  "categories": [
    "TestOps"
  ],
  "tags": [
    "testops"
  ],
  "permalink": "/2016/09/testops-2-testing-in-production",
  "content": "<img src=\"/images/blog/production-icon-18.png\" alt=\"\">\n\n<p>Some time ago I started <a href=\"https://awesome-testing.com/2016/07/testops-missing-piece-of-puzzle.html\" target=\"_blank\" rel=\"noreferrer\">the TestOps</a> series.<br>I outlined why the topic is important (actually I expect it to be even more important in the nearest future) and listed various topic which I&#39;d like to investigate deeper soon. Today the first from the list: Testing in Production.</p><p>I&#39;ll list prerequisites for successful TiP adoption and give you real-life examples from top companies. Let&#39;s begin with a definition though:</p><blockquote class=\"blog-quote\"><p>Testing in Production (TiP) is a set of software testing methodologies that utilises real users and production</p><p></p><p>environment in a way that both leverages the diversity of production, while mitigating risks to end users.</p><p></p></blockquote><p><a href=\"https://twitter.com/setheliot\" target=\"_blank\" rel=\"noreferrer\">Seth EliotÂ </a></p><h3>Prerequisites</h3>\n<p><strong>Mature engineering process</strong></p><p>Testing in production impacts your application running live, therefore it impacts your customers. You should always have this statement on the back of your head. There is no place for dummy data like $99,999.00 little mouse (funny example from Amazon below).</p><img src=\"/images/blog/4812.image_6EB04442.png\" alt=\"\">\n\n<p>Before you approach it you need to make sure you fully understand its impacts. You should have skilled people planning and executing it from start to finish. Usually when it comes to testing error margin is quite big, but that&#39;s not the case in TiP.</p><p><strong>Understanding and mitigating risks</strong></p><p>In the social media era once you show something on the Internet it stays there forever. However, there are also risks that are not so obvious. Be careful especially with performance/load tests. Even the slightest decrease in application speed may substantially <a href=\"http://www.webpagefx.com/blog/internet/website-page-load-time-conversions/\" target=\"_blank\" rel=\"noreferrer\">impact revenue &amp; conversion</a>.</p><p>What about your monitoring &amp; alerting? Would your tests trigger any false positives alarms? Do you even have possibilities to check how your tests impact performance metrics?</p><p>Have you thought about the data you collect? Once you generate fake traffic it may be corrupted. How can you then make data-driven decisions?</p><p><strong>Cleaning after yourself</strong></p><p>This is something you should have implemented in the testing environment too, but which is a must on production. Your tests may generate all kinds of useless stuff - users, database entries, fake data, logs. Make sure you erase them afterward. In case of logs add something to quickly identify a test. You don&#39;t want to spend time analyzing non-human traffic.</p><h3>Types of TiP</h3>\n<p><strong>Canary deployment &amp; Blue-Green deployment</strong></p><p>You deploy your software on a separate set of servers (Blue pool in the image below) and then slowly roll it out for customers. Start small (with internal or beta users) and do few smoke tests and log analyses. If everything seems ok redirect some part of external customers (5% in our example) for the new pool.</p><img src=\"/images/blog/canary-release-2.png\" alt=\"\">\n\n<p>Now assuming if something goes wrong you may rollback all traffic to sta able pool (Green) or just proceed with new release rollout and redirect half/all customers to Blue pool.</p><p>In case of a new software version you revert the roles, deployment is now on Green pool with Blue pool running as a safety net in case of unexpected bugs.</p><p>The goal here is to make the process transparent for customers.</p><p>On Martin Fowler&#39;s blog, you can find a more detailed and excellent description of those techniques: <a href=\"http://martinfowler.com/bliki/CanaryRelease.html\" target=\"_blank\" rel=\"noreferrer\">Canary release</a>, <a href=\"http://martinfowler.com/bliki/BlueGreenDeployment.html\" target=\"_blank\" rel=\"noreferrer\">Blue-Green deployment</a>.</p><p><strong>Controlled test flight</strong></p><p>The technique very similar to Canary deployment, but instead of a new application version we slowly roll out new features. I&#39;ll use <a href=\"http://aviadezra.blogspot.com/2014/05/testing-in-production-benefits-risks.html\" target=\"_blank\" rel=\"noreferrer\">Aviadezra</a> image to explain it in a simple way:</p><img src=\"/images/blog/image16.png\" alt=\"\">\n\n<p>Let&#39;s assume we have a new feature hidden in Code Path 1. After successful deployment, we have all customers using Code Path 2. Now we change the config file and from now on some customers (first internal, then 5%) are using a new feature that is visible in Code Path 1.</p><p>Once again we analyze how the application is performing, and if everything is ok we open Code Path 1 for all customers.<br>If we are disappointed by the results we rollback to Code Path 2.</p><p>The warning here: controlled test flight substantially increase application complexity. Business loves it though and in my opinion, it&#39;s worth the effort.</p><p><strong>A/B testing</strong></p><p>Another excellent tool for business and UX designers. I&#39;ll use <a href=\"https://vwo.com/ab-testing/\" target=\"_blank\" rel=\"noreferrer\">a vwo</a> image for an explanation (excellent guide, read it).</p><img src=\"/images/blog/02.png.pagespeed.ce.BmWcShEZAM.png\" alt=\"\">\n\n<p>The idea is very simple here. 50% of our customers see Variation A of our application, and 50% of customers see Variation B. We measure all the data and then analyze it (with Data Scientists help perhaps). In the case of our example, Variation A is better because it guarantees a higher conversion rate.</p><p>A/B testing is highly recommended by lean experts and you should utilize it even in startups.</p><p><strong>Synthetic User (Bot) Testing</strong></p><p>Synthetic user is a bot which runs real customer scenario on our application. To be more specific it&#39;s like an end-to-end (E2E) test running on a production environment. Tests don&#39;t have to be written using Selenium (as they&#39;re very often unstable) but should run popular journeys. Ideally, you should figure out scenarios using production data.</p><p>Bots may be triggered from various servers (ideally split geographically) and should be integrated with existing monitoring/alerting systems. Consecutive failures should trigger an investigation in your team.</p><p>Make sure you clean your data after each run (see prerequisite 3).</p><p><strong>Fault injection &amp; Recovery testing</strong></p><p>Technique popularised by <a href=\"http://techblog.netflix.com/2012/07/chaos-monkey-released-into-wild.html\" target=\"_blank\" rel=\"noreferrer\">Netflix&#39;s Chaos Monkey</a>. The idea is pretty simple - we generate random failures in our production infrastructure enforcing engineers to design recovery systems and develop a stronger, more adaptive platform.</p><blockquote class=\"blog-quote\"><p>The best defense against major unexpected failures is to fail often. By frequently causing failures, we force our</p><p></p><p>services to be built in a way that is more resilient</p><p></p></blockquote><p>Cory Bennett and Ariel Tseitlin - Netflix engineers</p><p><strong>Dogfooding</strong></p><p>Technique popularised by Microsoft which enforces usage of applications developed locally. For example, if your team is creating new GoogleDocs you force your developers to use it. A very clever way to improve the customer user experience.</p>"
}