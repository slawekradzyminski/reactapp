{
  "id": "2016-09-09-testops-2-testing-in-production",
  "title": "TestOps - Testing in Production",
  "date": "2016-09-09",
  "categories": [
    "TestOps"
  ],
  "tags": [
    "testops"
  ],
  "permalink": "/2016/09/testops-2-testing-in-production",
  "content": "<p><img src=\"/images/blog/production-icon-18.png\" alt=\"\" style=\"width: 100%;\"></p>\n<p>Some time ago I started <a href=\"https://awesome-testing.com/2016/07/testops-missing-piece-of-puzzle.html\" target=\"_blank\" rel=\"noreferrer\">the TestOps</a> series.<br>I outlined why the topic is important (actually I expect it to be even more important in the nearest future) and listed<br>various topic which I&#39;d like to investigate deeper soon. Today the first from the list: Testing in Production.</p>\n<p>I&#39;ll list prerequisites for successful TiP adoption and give you real-life examples from top companies. Let&#39;s begin with<br>a definition though:</p>\n<blockquote class=\"blog-quote\"><p>Testing in Production (TiP) is a set of software testing methodologies that utilises real users and production</p><p>environment in a way that both leverages the diversity of production, while mitigating risks to end users.  </p></p></blockquote><p><a href=\"https://twitter.com/setheliot\" target=\"_blank\" rel=\"noreferrer\">Seth EliotÂ </a></p>\n<h3>Prerequisites</h3>\n<p><strong>Mature engineering process</strong></p>\n<p>Testing in production impacts your application running live, therefore it impacts your customers. You should always have<br>this statement on the back of your head. There is no place for dummy data like $99,999.00 little mouse (funny example<br>from Amazon below).</p>\n<p><img src=\"/images/blog/4812.image_6EB04442.png\" alt=\"\" style=\"width: 100%;\"></p>\n<p>Before you approach it you need to make sure you fully understand its impacts. You should have skilled people planning<br>and executing it from start to finish. Usually when it comes to testing error margin is quite big, but that&#39;s not the<br>case in TiP.</p>\n<p><strong>Understanding and mitigating risks</strong></p>\n<p>In the social media era once you show something on the Internet it stays there forever. However, there are also risks<br>that are not so obvious. Be careful especially with performance/load tests. Even the slightest decrease in application<br>speed may<br>substantially <a href=\"http://www.webpagefx.com/blog/internet/website-page-load-time-conversions/\" target=\"_blank\" rel=\"noreferrer\">impact revenue &amp; conversion</a>.</p>\n<p>What about your monitoring &amp; alerting? Would your tests trigger any false positives alarms? Do you even have<br>possibilities to check how your tests impact performance metrics?</p>\n<p>Have you thought about the data you collect? Once you generate fake traffic it may be corrupted. How can you then make<br>data-driven decisions?</p>\n<p><strong>Cleaning after yourself</strong></p>\n<p>This is something you should have implemented in the testing environment too, but which is a must on production. Your<br>tests may generate all kinds of useless stuff - users, database entries, fake data, logs. Make sure you erase them<br>afterward. In case of logs add something to quickly identify a test. You don&#39;t want to spend time analyzing non-human<br>traffic.</p>\n<h3>Types of TiP</h3>\n<p><strong>Canary deployment &amp; Blue-Green deployment</strong></p>\n<p>You deploy your software on a separate set of servers (Blue pool in the image below) and then slowly roll it out for<br>customers. Start small (with internal or beta users) and do few smoke tests and log analyses. If everything seems ok<br>redirect some part of external customers (5% in our example) for the new pool.</p>\n<p><img src=\"/images/blog/canary-release-2.png\" alt=\"\" style=\"width: 100%;\"></p>\n<p>Now assuming if something goes wrong you may rollback all traffic to sta able pool (Green) or just proceed with new<br>release rollout and redirect half/all customers to Blue pool.</p>\n<p>In case of a new software version you revert the roles, deployment is now on Green pool with Blue pool running as a<br>safety net in case of unexpected bugs.</p>\n<p>The goal here is to make the process transparent for customers.</p>\n<p>On Martin Fowler&#39;s blog, you can find a more detailed and excellent description of those<br>techniques: <a href=\"http://martinfowler.com/bliki/CanaryRelease.html\" target=\"_blank\" rel=\"noreferrer\">Canary release</a>, <a href=\"http://martinfowler.com/bliki/BlueGreenDeployment.html\" target=\"_blank\" rel=\"noreferrer\">Blue-Green deployment</a>.</p>\n<p><strong>Controlled test flight</strong></p>\n<p>The technique very similar to Canary deployment, but instead of a new application version we slowly roll out new<br>features. I&#39;ll use <a href=\"http://aviadezra.blogspot.com/2014/05/testing-in-production-benefits-risks.html\" target=\"_blank\" rel=\"noreferrer\">Aviadezra</a> image to<br>explain it in a simple way:</p>\n<p><img src=\"/images/blog/image16.png\" alt=\"\" style=\"width: 100%;\"></p>\n<p>Let&#39;s assume we have a new feature hidden in Code Path 1. After successful deployment, we have all customers using Code<br>Path 2. Now we change the config file and from now on some customers (first internal, then 5%) are using a new feature<br>that is visible in Code Path 1.</p>\n<p>Once again we analyze how the application is performing, and if everything is ok we open Code Path 1 for all customers.<br>If we are disappointed by the results we rollback to Code Path 2.</p>\n<p>The warning here: controlled test flight substantially increase application complexity. Business loves it though and in<br>my opinion, it&#39;s worth the effort.</p>\n<p><strong>A/B testing</strong></p>\n<p>Another excellent tool for business and UX designers. I&#39;ll use <a href=\"https://vwo.com/ab-testing/\" target=\"_blank\" rel=\"noreferrer\">a vwo</a> image for an<br>explanation (excellent guide, read it).</p>\n<p><img src=\"/images/blog/02.png.pagespeed.ce.BmWcShEZAM.png\" alt=\"\" style=\"width: 100%;\"></p>\n<p>The idea is very simple here. 50% of our customers see Variation A of our application, and 50% of customers see<br>Variation B. We measure all the data and then analyze it (with Data Scientists help perhaps). In the case of our<br>example, Variation A is better because it guarantees a higher conversion rate.</p>\n<p>A/B testing is highly recommended by lean experts and you should utilize it even in startups.</p>\n<p><strong>Synthetic User (Bot) Testing</strong></p>\n<p>Synthetic user is a bot which runs real customer scenario on our application. To be more specific it&#39;s like an<br>end-to-end (E2E) test running on a production environment. Tests don&#39;t have to be written using Selenium (as they&#39;re<br>very often unstable) but should run popular journeys. Ideally, you should figure out scenarios using production data.</p>\n<p>Bots may be triggered from various servers (ideally split geographically) and should be integrated with existing<br>monitoring/alerting systems. Consecutive failures should trigger an investigation in your team.</p>\n<p>Make sure you clean your data after each run (see prerequisite 3).</p>\n<p><strong>Fault injection &amp; Recovery testing</strong></p>\n<p>Technique popularised<br>by <a href=\"http://techblog.netflix.com/2012/07/chaos-monkey-released-into-wild.html\" target=\"_blank\" rel=\"noreferrer\">Netflix&#39;s Chaos Monkey</a>. The idea is<br>pretty simple - we generate random failures in our production infrastructure enforcing engineers to design recovery<br>systems and develop a stronger, more adaptive platform.</p>\n<blockquote class=\"blog-quote\"><p>The best defense against major unexpected failures is to fail often. By frequently causing failures, we force our</p><p>services to be built in a way that is more resilient  </p></p></blockquote><p>Cory Bennett and Ariel Tseitlin - Netflix engineers</p>\n<p><strong>Dogfooding</strong></p>\n<p>Technique popularised by Microsoft which enforces usage of applications developed locally. For example, if your team is<br>creating new GoogleDocs you force your developers to use it. A very clever way to improve the customer user experience.</p>\n"
}