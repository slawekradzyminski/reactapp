{
  "id": "2016-12-29-system-traps-in-software-testing",
  "title": "System traps in software testing",
  "date": "2016-12-29",
  "category": "Testing thoughts",
  "permalink": "/2016/12/system-traps-in-software-testing",
  "content": "<img src=\"/images/blog/system.jpg\" alt=\"\">\n\n<p>I had written several times on this blog that from my perspective majority of testers today lack full Software Engineering Life-Cycle (SDLC) understanding. As a community we should also look at our projects not only through tester&#39;s eyes, but also acknowledge stakeholders goals &amp; needs (focusing mostly on customers). In the end our common goal is to make money for the company, not to ship bug-free software. Recently I read gripping book which may help in looking at software engineering from different angle: <a href=\"https://www.amazon.com/Thinking-Systems-Donella-H-Meadows/dp/1603580557\" target=\"_blank\" rel=\"noreferrer\">Thinking in Systems</a> by Donella H.<br>Meadows. It&#39;s not classic book about SDLC, but rather <a href=\"https://en.wikipedia.org/wiki/Systems_theory\" target=\"_blank\" rel=\"noreferrer\">systems theory</a><br>primer.</p><p>I strongly recommend reading Donella&#39;s book and those two blog posts - <a href=\"https://medium.com/@Smrimell/it-s-a-trap-systems-traps-in-software-development-dc6341022795#.62m17ayj0\" target=\"_blank\" rel=\"noreferrer\">Part I</a>, <a href=\"https://medium.com/@Smrimell/escape-the-trap-avoiding-traps-in-software-development-with-systems-thinking-cbc20af7c719#.d6nlbrtes\" target=\"_blank\" rel=\"noreferrer\">Part II</a><br>which extend chapter 5 of the book. Having understood those I figured out I would try to show the same traps in software testing. All those examples are based on my personal experience, books, blogs and chats with other testers. All quotes below are from sources listed above.</p><h2>Trap 1 - Policy Resistance: separate development teams and testing team</h2>\n<blockquote class=\"blog-quote\"><p>Otherwise known as ‘Fixes that fail’, this trap prevents system change due to the conflicting needs of multiple actors</p><p></p><p>in the system.</p><p></p></blockquote><p>Imagine you are creating a complicated system which require multiple parts developed by separate teams. You know that integration will be painful, so you decide to create new team responsible for end-to-end system testing. Unfortunately as soon as they start working you realise that most of the integrations don&#39;t work and they struggle to pass even the most basic test case. What happened?</p><p>By creating separate team you reduced product responsibility from development teams. In such case they were focusing more on meeting deadlines at their end than on common goal - making it work for end customer. You also started waterfall-like process in which teams think only on basic level testing (unit, integration) without E2E perspective.<br>Performance? Let&#39;s leave it to system testing team. They will check that.</p><p>Worst of all this separate team of testers which now tries to prove it&#39;s usefulness by focusing on finding as many bugs a possible without thinking of improving the whole process/product.</p><p>I understand that having system testing team is sometimes necessary, but always pay close attention how it cooperates with others.</p><h2>Trap 2 - Tragedy on the commons: disposable, temporary tester</h2>\n<blockquote class=\"blog-quote\"><p>Tragedy of the commons is a systems archetype that describes an escalation in the usage of a common resource,</p><p></p><p>eventually resulting in its depletion and ultimate destruction.</p><p></p></blockquote><p>Let&#39;s say you don&#39;t want to invest too much into quality, so you figure out that your testers would temporarily join projects which require them most. The idea is very alluring, but the results not so much. Developers know that their code would be tested a some point, but they don&#39;t think about quality on daily basis. Again, we have waterfall-like process. When something is ready for testing you call Mr. Tester, and he joins the project.</p><p>At first, it takes some time for Mr. Tester to read all the documentation and requirements. He then performs manual testing which can&#39;t really be called exploratory, because he&#39;s application knowledge is nowhere near being expert (those are all basic checks really). Soon he&#39;s more familiar with a team and project he&#39;s ready to start improve the process and automate test cases. But what happens? He&#39;s delegated to different project and everything starts again.</p><p>If you ever want to get rid of testers in your organisation this is a way to go. They&#39;ll be so demotivated after working in such style that they will leave on first opportunity. Tester is not a commodity.</p><h2>Trap 3 - Drift to low performance: getting used to poor tests</h2>\n<blockquote class=\"blog-quote\"><p>Drift to low performance describes a trap where a system not only resists positive change, but continually drifts</p><p></p><p>towards poorer performance.</p><p></p></blockquote><p>Failed tests? Ignore them. Flaky tests? Ignore them. I&#39;m pretty sure every tester know what I&#39;m talking about. This is why reliability of automated test suite is crucial. If tests start to fail developers care less and less ignoring them at some point entirely. Worst of all even valid findings are getting lost in flakiness noise.</p><p>This is classic example of drift to low performance and by doing nothing about it situation quickly gets out of control.<br>As a tester you should act fast. Identify issues, prepare a plan and involve product owner into task prioritisation. You shouldn&#39;t work alone on fixing that (quality is everyone responsibility).</p><p>Google Testing Blog has nice post about <a href=\"https://testing.googleblog.com/2016/05/flaky-tests-at-google-and-how-we.html\" target=\"_blank\" rel=\"noreferrer\">flakiness</a>.</p><h2>Trap 4 - Escalation: rating testers by number of bugs found</h2>\n<blockquote class=\"blog-quote\"><p>This trap is about keeping slightly ahead of the Joneses.</p><p></p></blockquote><p>Perfect sub-optimisation example - you start rating your local or outsourced testers by number of bugs reported. If you add priorities into the mix it&#39;s even worse. You end up with dozens of over-prioritised tickets and daily developer vs tester arguments.</p><p>Not a great way to create common goal mindset. Don&#39;t do it. No exceptions here.</p><p>If you really want to somehow rate your individual tester ask people he works with and gather feedback. <a href=\"https://en.wikipedia.org/wiki/360-degree_feedback\" target=\"_blank\" rel=\"noreferrer\">360-degree feedback</a> is the most popular way of doing it.</p><h2>Trap 5 - Success to the successful: giving up testing improvement in legacy apps</h2>\n<blockquote class=\"blog-quote\"><p>The rich get richer while the poor stay poor.</p><p></p></blockquote><p>It&#39;s easy to say that monolithic applications can&#39;t be tested in effective, unreliable and fast way. A lot of people at this point start dreaming about microservices migration and future in which everything would be much easier. What about now? Why bother? It would better soon.</p><p>Well, no. Testers should improve continuously having in mind what&#39;s now and what we release for customers now. Are you testing GUI or through GUI? Are your test following test pyramid as much as possible? Do you setup your tests in most effective way? Are your tests covering as much as possible?</p><p>There is so much things we can improve daily that we should never ever give up on that. Poor don&#39;t always have to be poor.</p><h2>Trap 6 - Shifting the burden to the intervenor: blaming tester for poor quality</h2>\n<blockquote class=\"blog-quote\"><p>Addiction occurs when reliance on a particular solution to a problem undermines the capacity of the system to support</p><p></p><p>itself.</p><p></p></blockquote><p>In ancient times it was dangerous to report bad news. Kings in their delusional rage may have ordered to kill you, even when you were just messenger. Surprisingly scapegoating is still popular, and as a testers we may be in danger.</p><p>Not long ago someone thought that testers should be &#39;promoted&#39; into quality keeper role. He certainly didn&#39;t read Donella&#39;s systems theory book, cause the experiment failed dramatically. Developers had a perfect excuse not to test too much. It was quality keeper responsibility to make sure their code works. And if something went wrong on production?<br>Blame tester. He didn&#39;t find the bug.</p><p>Once again - quality is everyone responsibility. Quality is also the result of complicated system and no single person has the power to improve it alone.</p><h2>Trap 7 - Rule beating: dummy Definition of Done activities</h2>\n<blockquote class=\"blog-quote\"><p>You have a problem with rule beating when you find something stupid happening because its a way around the rule.</p><p></p></blockquote><p><a href=\"https://www.scrumalliance.org/community/articles/2008/september/what-is-definition-of-done-(dod)\" target=\"_blank\" rel=\"noreferrer\">Definition of Done</a> -<br>very useful checklist in determining if we have finished our work on given feature or not. DoD should be very carefully written though (tester should be actively involved in creating it btw). Don&#39;t add unreal amount of testing until you are ready from automation point of view.</p><p>Let&#39;s say you check you data and find 5 most popular desktop resolutions, 5 browsers and 5 mobile devices. Even with this relatively low number it&#39;s almost impossible to cover it manually. Before adding it to DoD create visual verification test suites with <a href=\"http://galenframework.com/\" target=\"_blank\" rel=\"noreferrer\">Galen Framework</a><br>or <a href=\"https://github.com/garris/BackstopJS\" target=\"_blank\" rel=\"noreferrer\">BackstopJS</a> and run functional tests on various browser. I don&#39;t recommend doing it in opposite way, because most of the manual checks would be omitted entirely, or done just to follow the law with resentment.</p><p>I recommend creating a document of supported configurations and let teams decide the correct amount of testing. Risk assessment is much better than dummy activities just to follow the bad rules.</p><h2>Trap 8 - Seeking the wrong goal: quality by numbers</h2>\n<blockquote class=\"blog-quote\"><p>This trap often occurs when one’s progress towards a goal is evaluated via a proxy measure rather than the goal</p><p></p><p>itself.</p><p></p></blockquote><p>With 90% code coverage we can be sure that everything works well. Well, sort of. If you enforce level of coverage bad things start to happen. Developers write tests not because they think it&#39;s useful, but because they need to reach magic number required by code coverage tool. I don&#39;t like this approach. I may be little naive, but I like to let others decide how to their job in the best possible way.</p><p>Instead of enforcing a value testers should show the developers value of extensive unit tests safety net. Interestingly enough most of the programmers don&#39;t want to work with legacy code. It&#39;s hard to refactor the code without unit tests though... so conclusion is obvious.</p><p>From my personal observations senior software engineers write a lot more unit tests than beginners. With their vast experience they understand the value without any quality gates.</p><p>The only reliable product quality metric can be customer satisfaction and amout of money you made selling your products.</p>"
}