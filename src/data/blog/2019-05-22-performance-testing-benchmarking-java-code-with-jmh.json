{
  "id": "2019-05-22-performance-testing-benchmarking-java-code-with-jmh",
  "title": "Performance testing (benchmarking) Java code with JMH",
  "date": "2019-05-22",
  "categories": [
    "Performance testing"
  ],
  "tags": [
    "performance testing"
  ],
  "permalink": "/2019/05/performance-testing-benchmarking-java",
  "content": "<p><img src=\"/images/blog/performance-icon-png-8.png\" alt=\"\" style=\"width: 100%;\"></p>\n<p>As test engineers when we approach performance testing we usually only think about final end-to-end application<br>verification with tools such as <a href=\"https://jmeter.apache.org/\" target=\"_blank\" rel=\"noreferrer\">JMeter</a>, <a href=\"https://locust.io/\" target=\"_blank\" rel=\"noreferrer\">Locust</a><br>or <a href=\"https://gatling.io/\" target=\"_blank\" rel=\"noreferrer\">Gatling</a>. We know that such tests should run on a separate environment with conditions<br>resembling production as close as possible. Unfortunately in some cases (especially with monolithic architecture)<br>dedicated performance testing environment is hard to get. What to do in such cases? Should we test on common test<br>environment? Or should we test on production? Or maybe we should change our approach to performance testing? Each option<br>has advantages and disadvantages.</p>\n<p>Today I&#39;d like to describe low-level performance testing (often called benchmarking) of Java code. It does not require a<br>separate environment. It can be executed directly from your IDE (although that&#39;s not recommended) or from the command<br>line. Measuring the performance of critical pieces of code is essential for everyone who creates applications,<br>frameworks, and tools. Testers are co-creators so it&#39;s also our responsibility.</p>\n<h2>Is it easy?</h2>\n<p>Benchmarking correctly is hard. There are multiple optimizations implemented on the JVM/OS/hardware side which make it<br>challenging. In order to measure right, you need to understand how to avoid those optimizations because they may not<br>happen in the real production system.</p>\n<p>Thankfully, there is a tool which helps you mitigate those issues called JMH (Java Microbenchmark Harness). It was<br>created for building, running, and analyzing nano/micro/milli/macro benchmarks written in Java and other languages<br>targetting the JVM. The tool is developed by <a href=\"http://openjdk.java.net/projects/code-tools/jmh/\" target=\"_blank\" rel=\"noreferrer\">OpenJDK</a>.</p>\n<h2>Common pitfalls</h2>\n<p>JMH tool does not guarantee that your benchmark is implemented correctly. You still need to avoid common pitfalls.<br>According to tool creators, the best way to learn how to use JMH is to read through the official JMH samples before<br>implementing your own benchmarks.</p>\n<p>I&#39;ll describe only the four most common pitfalls. For a more complete list please refer to other sources(see the further<br>reading section at the bottom).</p>\n<p><strong><em>a) Dead code elimination</em></strong></p>\n<p>JVM is smart enough to detect that certain code is never used. That&#39;s why the methods you measure should always return<br>something. Alternatively, you can use JMH consume method which guarantees that consumed code will never be buried by<br>JVM.</p>\n<pre><code class=\"language-java\">@Benchmark\n   public void test(Blackhole blackhole) {\n        int a = 2;\n        int b = 1;\n        int sum = a - b;\n        blackhole.consume(sum);\n    }\n</code></pre>\n<p><strong><em>b) Constant folding</em></strong></p>\n<p>If JVM realizes the result of the computation is the same no matter what, it can cleverly optimize it. That&#39;s why you<br>should act against your IDE suggestion and don&#39;t make any fields final.</p>\n<pre><code class=\"language-java\">private double x = Math.PI;\n    private final double wrongX = Math.PI;\n\n    @Benchmark\n    public double wrong() {\n        return Math.log(wrongX);\n    }\n\n    @Benchmark\n    public double right() {\n        return Math.log(x);\n    }\n</code></pre>\n<p><em><strong>c) Loop optimizations</strong></em></p>\n<p>You need to be very careful benchmarking unit operations within loops and dividing measurements by the number of<br>iterations. JVM optimizes the loop, so the cost of the loop is smaller than the sum of the costs of its parts measured<br>in isolation.</p>\n<p>It&#39;s a bit tricky (I have misunderstood how it works at first) so I suggest to take a closer look at JMH<br>example <a href=\"https://github.com/Valloric/jmh-playground/blob/master/src/jmh/java/org/openjdk/jmh/samples/JMHSample_11_Loops.java\" target=\"_blank\" rel=\"noreferrer\">11</a><br>and <a href=\"https://github.com/Valloric/jmh-playground/blob/master/src/jmh/java/org/openjdk/jmh/samples/JMHSample_34_SafeLooping.java\" target=\"_blank\" rel=\"noreferrer\">34</a>.</p>\n<p><strong><em>d) Warmup</em></strong></p>\n<p>From official <a href=\"https://github.com/Valloric/jmh-playground\" target=\"_blank\" rel=\"noreferrer\">JMH playground GitHub</a> repository:</p>\n<ul>\n<li>You need warmup iterations because of JVM and JIT warmup. How many depends on the benchmark, but probably no less than<ol start=\"5\">\n<li>A safer number is 10.</li>\n</ol>\n</li>\n<li>The more measurement iterations you use, the smaller the error margin reported by JMH at the end! A solid choice is 20<br>iterations.</li>\n</ul>\n<h2>Setup</h2>\n<p>JMH is very easy to start working with. You can create your own project in a couple of seconds. Just execute the<br>following command in your ~/IdeaProjects folder and open first-benchmark project.</p>\n<p><code>mvn archetype:generate -DinteractiveMode=false -DarchetypeGroupId=org.openjdk.jmh -DarchetypeArtifactId=jmh-java-benchmark-archetype -DgroupId=com.awesome.testing -DartifactId=first-benchmark -Dversion=1.21</code></p>\n<p>Alternatively, if you want to create tests in existing project, you can follow the same steps I did for<br>my <a href=\"https://github.com/slawekradzyminski/AwesomeTesting\" target=\"_blank\" rel=\"noreferrer\">AwesomeTesting Github</a> repository. First, add maven<br>dependencies. In a real-world scenario you probably also need to add your application here(in order to call methods you<br>want to measure).</p>\n<pre><code class=\"language-xml\">&lt;dependency&gt;\n            &lt;groupId&gt;org.openjdk.jmh&lt;/groupId&gt;\n            &lt;artifactId&gt;jmh-core&lt;/artifactId&gt;\n            &lt;version&gt;1.21&lt;/version&gt;\n        &lt;/dependency&gt;\n        &lt;dependency&gt;\n            &lt;groupId&gt;org.openjdk.jmh&lt;/groupId&gt;\n            &lt;artifactId&gt;jmh-generator-annprocess&lt;/artifactId&gt;\n            &lt;version&gt;1.21&lt;/version&gt;\n        &lt;/dependency&gt;\n</code></pre>\n<p>After that you need to configure maven-shade-plugin which will be responsible for creating an executable jar.</p>\n<pre><code class=\"language-xml\">&lt;plugin&gt;\n                &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt;\n                &lt;artifactId&gt;maven-shade-plugin&lt;/artifactId&gt;\n                &lt;version&gt;3.2.1&lt;/version&gt;\n                &lt;executions&gt;\n                    &lt;execution&gt;\n                        &lt;goals&gt;\n                            &lt;goal&gt;shade&lt;/goal&gt;\n                        &lt;/goals&gt;\n                        &lt;configuration&gt;\n                            &lt;finalName&gt;benchmarks&lt;/finalName&gt;\n                            &lt;transformers&gt;\n                                &lt;transformer\n                                        implementation=&quot;org.apache.maven.plugins.shade.resource.ManifestResourceTransformer&quot;&gt;\n                                    &lt;manifestEntries&gt;\n                                        &lt;Main-Class&gt;org.openjdk.jmh.Main&lt;/Main-Class&gt;\n                                        &lt;X-Compile-Source-JDK&gt;1.8&lt;/X-Compile-Source-JDK&gt;\n                                        &lt;X-Compile-Target-JDK&gt;1.8&lt;/X-Compile-Target-JDK&gt;\n                                    &lt;/manifestEntries&gt;\n                                &lt;/transformer&gt;\n                            &lt;/transformers&gt;\n                            &lt;filters&gt;\n                                &lt;filter&gt;\n                                    &lt;artifact&gt;*:*&lt;/artifact&gt;\n                                    &lt;excludes&gt;\n                                        &lt;exclude&gt;META-INF/*.SF&lt;/exclude&gt;\n                                        &lt;exclude&gt;META-INF/*.DSA&lt;/exclude&gt;\n                                        &lt;exclude&gt;META-INF/*.RSA&lt;/exclude&gt;\n                                    &lt;/excludes&gt;\n                                &lt;/filter&gt;\n                            &lt;/filters&gt;\n                        &lt;/configuration&gt;\n                    &lt;/execution&gt;\n                &lt;/executions&gt;\n            &lt;/plugin&gt;\n</code></pre>\n<p>Now create your jar:</p>\n<p><code>mvn clean package -DskipTests=true</code></p>\n<p>And execute benchmarks (provide the class name which contains @Benchmark annotated methods)</p>\n<p><code>java -jar target/benchmarks.jar BenchMark</code></p>\n<h2>Configuration</h2>\n<p>There are three alternative ways of configuring JMH:</p>\n<p><strong><em>a) Annotations</em></strong></p>\n<p>You can use Java annotations which is very convenient. That&#39;s my favorite type by far because I can easily access nicely<br>written Javadoc for each option. Here is how I configured my demo (each option would be described below):</p>\n<pre><code class=\"language-java\">@Fork(value = 3, warmups = 0)\n@OutputTimeUnit(TimeUnit.MILLISECONDS)\n@BenchmarkMode(Mode.AverageTime)\n@Warmup(iterations = 5, time = 10)\n@Measurement(iterations = 5, time = 10)\npublic class BenchMark {\n</code></pre>\n<p><strong><em>b) Java API</em></strong></p>\n<p>If you prefer Java API you can use builder configuration and run your tests from IDE. That&#39;s not recommended because<br>Benchmark should ideally run with every single application closed.</p>\n<pre><code class=\"language-java\">public static void main(String[] args) throws RunnerException {\n        Options opt = new OptionsBuilder()\n                .include(BenchMark.class.getSimpleName())\n                .warmupIterations(5)\n                .measurementIterations(5)\n                .forks(1)\n                .build();\n\n        new Runner(opt).run();\n    }\n</code></pre>\n<p><strong><em>c) Command line</em></strong></p>\n<p>Last but not least, you can the command line to override configuration. See help for more info:</p>\n<p><code>java -jar target/benchmarks.jar BenchMark -h</code></p>\n<p>Example: 5 warmup/measurement iterations, single fork</p>\n<p><code>java -jar target/benchmarks.jar BenchMark -wi 5 -i 5 -f 1</code></p>\n<h2>Configuration options</h2>\n<p>JMH provides multiple configuration options which can be used in our benchmarks. You can find detailed descriptions in<br>excellent JMH Javadoc.</p>\n<p><strong><em>a) BenchmarkMode</em></strong></p>\n<p>Defines the mode in which this benchmark will run.</p>\n<ul>\n<li>Throughput: measures the number of operations per time unit</li>\n<li>AverageTime: measures the average time it takes for the benchmark method to execute (timeunit/operation)</li>\n<li>SampleTime: samples the time for each operation (min, max, etc.)</li>\n<li>SingleShotTime:measures the time for a single operation.</li>\n<li>All: All modes</li>\n</ul>\n<p><em><strong>b) OutputTimeUnit</strong></em></p>\n<p>Defines a time-unit for results (nanoseconds, milliseconds, seconds, minutes, etfc.)</p>\n<p><em><strong>c) Fork</strong></em></p>\n<p>The fork allows setting the default forking parameters for the benchmark.</p>\n<ul>\n<li>value() defines how many times the benchmark will execute</li>\n<li>warmups() defines how many time the benchmark will execute ignoring results</li>\n</ul>\n<p>Additionally, you can modify JVM options using this annotation.</p>\n<p><em><strong>d) Warmup</strong></em></p>\n<p>Warmup allows setting the default warmup parameters for the benchmark which will execute before each fork (including<br>warmup forks).</p>\n<ul>\n<li>iterations() sets the number of iterations</li>\n<li>time() sets the time for each iteration</li>\n<li>batchSize() sets the number of benchmark method calls per operation</li>\n</ul>\n<p><em><strong>e) Measurement</strong></em></p>\n<p>Measurement allow setting the default measurement parameters for the actual benchmark. Parameters the same as @Warmup.</p>\n<p><strong><em>f) Threads</em></strong></p>\n<p>Threads provides the default number of threads to run. By default, this setting is set to 1. If you want to use maximum<br>set value to Threads.MAX (the equivalent of <em>Runtime.getRuntime().availableProcessors()</em>)</p>\n<h2>Predefining state</h2>\n<p>Unfortunately, two sections of this article aren&#39;t enough to fully describe the JMH configuration. One of the most<br>challenging tasks when it comes to benchmarking is ensuring clean and not JVM-optimized state before each measurement.</p>\n<p>Of course, JMH helps us in this domain as well with class-level @State annotation. This has to be nested public static<br>class defined in a class which contains our Benchmark methods.</p>\n<p>The Scope class gives us the following options:</p>\n<ul>\n<li>Thread: Each thread running the benchmark will create its own instance of the state object.</li>\n<li>Group: Each thread group running the benchmark will create its own instance of the state object.</li>\n<li>Benchmark: All threads running the benchmark share the same state object.</li>\n</ul>\n<p>Having State class in place we can now use @Setup (the equivalent of Junit/TestNG Before) and @TearDown (Junit/TestNG<br>After) which will execute before/after @Benchmark methods. The execution time of these methods will not be counted in<br>our benchmark results.</p>\n<p>Setup and Teardown can be set for three levels:</p>\n<ul>\n<li>Trial: before/after each fork</li>\n<li>Iteration: before/after each iteration</li>\n<li>Invocation: before/after measured method invocation. Javadoc for this option starts with WARNING: HERE BE DRAGONS! so<br>unless you want to meet Game of Thrones dragons don&#39;t use it</li>\n</ul>\n<p>In order to fully understand @Scope, you need to take a look into an actual example.</p>\n<p>Let&#39;s suppose we want to measure multiplication in 4 scenarios (1*1, 1*31, 31*1, 31*31). We also want to start each<br>fork with 0 as a result. After each iteration, we want to do garbage collection. Here is how our benchmark should look<br>like:</p>\n<pre><code class=\"language-java\">public class ExplainingState {\n\n    @State(Scope.Thread)\n    public static class PredefinedState {\n\n        @Param({&quot;1&quot;, &quot;31&quot;})\n        public int a;\n\n        @Param({&quot;1&quot;, &quot;31&quot;})\n        public int b;\n\n        @Setup(Level.Trial)\n        public void doForkSetup() {\n            multiplicationResult = 0;\n            System.out.println(&quot;\\n Do Setup before each fork \\n&quot;);\n        }\n\n        @Setup(Level.Iteration)\n        public void doIterationSetup() {\n            System.out.println(&quot;\\n Do Setup before each iteration \\n&quot;);\n        }\n\n        @TearDown(Level.Iteration)\n        public void doIterationTeardown() {\n            System.out.println(&quot;\\n Do teardown after each iteration \\n&quot;);\n            System.gc();\n        }\n\n        public int multiplicationResult ;\n    }\n\n    @BenchmarkMode(Mode.Throughput)\n    @OutputTimeUnit(TimeUnit.MINUTES)\n    @Fork(value = 1, warmups = 0)\n    @Measurement(time = 1, iterations = 3)\n    @Benchmark\n    public void testMethod(PredefinedState predefinedState) {\n        predefinedState.multiplicationResult = predefinedState.a * predefinedState.b;\n    }\n}\n</code></pre>\n<p>If you still don&#39;t understand please run the benchmark and analyze console output in order to see what&#39;s going on and<br>when.</p>\n<h2>Demo</h2>\n<p>Uff... finally, demo time! After this long introduction, we can do something interesting. Let&#39;s say we want to measure<br>how fast different implementations of methods summing 20000000 longs work. It&#39;s 1 + 2 + 3 +... + 20000000.</p>\n<p>We have 5 contenders (full credit for problem definition goes<br>to <a href=\"https://www.amazon.com/Modern-Java-Action-functional-programming/dp/1617293563\" target=\"_blank\" rel=\"noreferrer\">Modern Java in Action</a> book which I<br>can recommend).</p>\n<p><strong><em>a) iterativeSum()</em></strong></p>\n<pre><code class=\"language-java\">@Benchmark\n    public long iterativeSum() {\n        long result = 0;\n        for (long i = 1L; i &lt;= N; i++) {\n            result += i;\n        }\n        return result;\n    }\nview raw\n</code></pre>\n<p><strong><em>b) sequentialSum()</em></strong></p>\n<pre><code class=\"language-java\">@Benchmark\n    public long sequentialSum() {\n        return Stream.iterate(1L, i -&gt; i + 1)\n                .limit(N)\n                .reduce(0L, Long::sum);\n    }\n</code></pre>\n<p><strong><em>c) parallelSum()</em></strong></p>\n<pre><code class=\"language-java\">@Benchmark\n    public long parallelSum() {\n        return Stream.iterate(1L, i -&gt; i + 1)\n                .parallel()\n                .limit(N)\n                .reduce(0L, Long::sum);\n    }\n</code></pre>\n<p><strong><em>d) rangedSum()</em></strong></p>\n<pre><code class=\"language-java\">@Benchmark\n    public long rangedSum() {\n        return LongStream.rangeClosed(1, N)\n                .reduce(0L, Long::sum);\n    }\n</code></pre>\n<p><strong><em>e) parallelRangedSum()</em></strong></p>\n<pre><code class=\"language-java\">@Benchmark\n    public long parallelRangedSum() {\n        return LongStream.rangeClosed(1, N)\n                .parallel()\n                .reduce(0L, Long::sum);\n    }\n</code></pre>\n<p>What do you think which implementation would be the best? We can theorize that implementations 2 and 3 would be slower<br>because of <a href=\"https://docs.oracle.com/javase/tutorial/java/data/autoboxing.html\" target=\"_blank\" rel=\"noreferrer\">autoboxing</a>, but let&#39;s see!</p>\n<h2>Results</h2>\n<p>So here are result on my Mac with Intel(R) Core(TM) i7-4870HQ CPU @ 2.50GHz:</p>\n<p>Setup:</p>\n<p><img src=\"/images/blog/Screen%2BShot%2B2019-05-25%2Bat%2B14.51.04.png\" alt=\"\" style=\"width: 100%;\"></p>\n<p>Measurements:</p>\n<p><img src=\"/images/blog/Screen%2BShot%2B2019-05-25%2Bat%2B14.50.18.png\" alt=\"\" style=\"width: 100%;\"></p>\n<p>Verdict:paralledRangedSum() has won the contest :)</p>\n<p>As a homework you can try to parametrise N and see if parallelRangedSum() is still the best.</p>\n<h2>Further reading</h2>\n<ul>\n<li><a href=\"https://www.amazon.com/Modern-Java-Action-functional-programming/dp/1617293563\" target=\"_blank\" rel=\"noreferrer\">Modern Java in Action</a></li>\n<li><a href=\"https://www.baeldung.com/java-microbenchmark-harness\" target=\"_blank\" rel=\"noreferrer\">https://www.baeldung.com/java-microbenchmark-harness</a></li>\n<li><a href=\"https://github.com/Valloric/jmh-playground\" target=\"_blank\" rel=\"noreferrer\">https://github.com/Valloric/jmh-playground</a></li>\n<li><a href=\"http://tutorials.jenkov.com/java-performance/jmh.html\" target=\"_blank\" rel=\"noreferrer\">http://tutorials.jenkov.com/java-performance/jmh.html</a></li>\n<li><a href=\"https://openjdk.java.net/projects/code-tools/jmh/\" target=\"_blank\" rel=\"noreferrer\">https://openjdk.java.net/projects/code-tools/jmh/</a></li>\n<li><a href=\"https://www.oracle.com/technetwork/articles/java/architect-benchmarking-2266277.html\" target=\"_blank\" rel=\"noreferrer\">https://www.oracle.com/technetwork/articles/java/architect-benchmarking-2266277.html</a></li>\n<li><a href=\"https://shipilev.net/\" target=\"_blank\" rel=\"noreferrer\">https://shipilev.net</a></li>\n<li><a href=\"https://shipilev.net/talks/jvmls-July2014-benchmarking.mp4\" target=\"_blank\" rel=\"noreferrer\">https://shipilev.net/talks/jvmls-July2014-benchmarking.mp4</a></li>\n<li><a href=\"https://www.youtube.com/watch?v=VaWgOCDBxYw\" target=\"_blank\" rel=\"noreferrer\">https://www.youtube.com/watch?v=VaWgOCDBxYw</a></li>\n<li><a href=\"https://shipilev.net/talks/j1-Oct2011-21682-benchmarking.pdf\" target=\"_blank\" rel=\"noreferrer\">https://shipilev.net/talks/j1-Oct2011-21682-benchmarking.pdf</a></li>\n<li><a href=\"http://leogomes.github.io/assets/JMH_cheatsheet.pdf\" target=\"_blank\" rel=\"noreferrer\">http://leogomes.github.io/assets/JMH_cheatsheet.pdf</a></li>\n</ul>\n"
}